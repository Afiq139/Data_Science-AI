{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# data, info = tfds.load('ag_news_subset', with_info='True', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = info.features['label'].names\n",
    "# class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert data to list\n",
    "# training_sentences = []\n",
    "# training_labels = []\n",
    "\n",
    "# for s, l in data['train']:\n",
    "#     training_sentences.append(str(s.numpy().decode()))\n",
    "#     training_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_sentences = []\n",
    "# testing_labels = []\n",
    "\n",
    "# for s, l in data['test']:\n",
    "#     testing_sentences.append(str(s.numpy().decode()))\n",
    "#     testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-re-install\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocess_kgptalkie as ps\n",
    "# import re\n",
    "\n",
    "# def get_clean(x):\n",
    "#     x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n",
    "#     x = ps.cont_exp(x)\n",
    "#     x = ps.remove_emails(x)\n",
    "#     x = ps.remove_urls(x)\n",
    "#     x = ps.remove_html_tags(x)\n",
    "#     x = ps.remove_accented_chars(x)\n",
    "#     x = ps.remove_special_chars(x)\n",
    "#     x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_clean(training_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sentences = [get_clean(x) for x in training_sentences]\n",
    "# testing_sentences = [get_clean(x) for x in testing_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data_lens = [len(x.split(\" \")) for x in training_sentences]\n",
    "# plt.hist(data_lens, bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 50_000\n",
    "# input_length = 50\n",
    "\n",
    "# tokenizer = Tokernizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "# tokernizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# training_sequences = tokernizer.text_to_sequences(training_sentences)\n",
    "# training_padded = pad_sequences(training_sequences, padding='post', truncating='post', maxlen=input_length)\n",
    "\n",
    "# testing_sequences = tokernizer.text_to_sequences(testing_sentences)\n",
    "# testing_padded = pad_sequences(testing_sequences, padding='post', truncating='post', maxlen=input_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense, Embedding\n",
    "# from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(class_names)\n",
    "# embedding_dim = 16\n",
    "# sequence_length = 50\n",
    "\n",
    "# y_train_encoded = to_categorical(training_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define LSTM Model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, imput_length=sequence_length))\n",
    "# model.add(LSTM(units=embedding_dim, dropout=0.2, return_sequences=True))\n",
    "# model.add(LSTM(units=embedding_dim, dropout=0.2))\n",
    "\n",
    "# model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(training_padded, y_train_padded, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM BiLSTM (pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, imput_length=sequence_length))\n",
    "# model.add(Bidirectional(LSTM(units=embedding_dim, dropout=0.2, return_sequences=True)))\n",
    "# model.add(Bidirectional(LSTM(units=embedding_dim, dropout=0.2)))\n",
    "\n",
    "# model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(training_padded, y_train_padded, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loading & Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('news_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer1=pickle.load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = load_model('news_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text= [\"i am retiring from the cricket. sachin is god of the game\"]\n",
    "## text= [\"i am not happy with the service\"]\n",
    "# text[0] = get_clrean(text[0])\n",
    "\n",
    "# seq = tokenizer.texts_to_sequences(text)\n",
    "# padded = pad_sequences(seq, maxlen=sequence_length)\n",
    "# pred = model.predict(padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_sentences[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
