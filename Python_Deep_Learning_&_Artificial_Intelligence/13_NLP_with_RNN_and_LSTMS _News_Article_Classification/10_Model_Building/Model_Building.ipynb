{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# data, info = tfds.load('ag_news_subset', with_info='True', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = info.features['label'].names\n",
    "# class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News Dataset Preparation Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert data to list\n",
    "# training_sentences = []\n",
    "# training_labels = []\n",
    "\n",
    "# for s, l in data['train']:\n",
    "#     training_sentences.append(str(s.numpy().decode()))\n",
    "#     training_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_sentences = []\n",
    "# testing_labels = []\n",
    "\n",
    "# for s, l in data['test']:\n",
    "#     testing_sentences.append(str(s.numpy().decode()))\n",
    "#     testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-re-install\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocess_kgptalkie as ps\n",
    "# import re\n",
    "\n",
    "# def get_clean(x):\n",
    "#     x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n",
    "#     x = ps.cont_exp(x)\n",
    "#     x = ps.remove_emails(x)\n",
    "#     x = ps.remove_urls(x)\n",
    "#     x = ps.remove_html_tags(x)\n",
    "#     x = ps.remove_accented_chars(x)\n",
    "#     x = ps.remove_special_chars(x)\n",
    "#     x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_clean(training_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sentences = [get_clean(x) for x in training_sentences]\n",
    "# testing_sentences = [get_clean(x) for x in testing_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data_lens = [len(x.split(\" \")) for x in training_sentences]\n",
    "# plt.hist(data_lens, bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 50_000\n",
    "# input_length = 50\n",
    "\n",
    "# tokenizer = Tokernizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "# tokernizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# training_sequences = tokernizer.text_to_sequences(training_sentences)\n",
    "# training_padded = pad_sequences(training_sequences, padding='post', truncating='post')\n",
    "\n",
    "# testing_sequences = tokernizer.text_to_sequences(testing_sentences)\n",
    "# testing_padded = pad_sequences(testing_sequences, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense, Embedding\n",
    "# from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(class_names)\n",
    "# embedding_dim = 16\n",
    "# sequence_length = 50\n",
    "\n",
    "# y_train_encoded = to_categorical(training_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define LSTM Model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, imput_length=sequence_length))\n",
    "# model.add(LSTM(units=embedding_dim, dropout=0.2, return_sequences=True))\n",
    "# model.add(LSTM(units=embedding_dim, dropout=0.2))\n",
    "\n",
    "# model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
